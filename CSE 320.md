# CMU-15-213-Intro-to-Computer-Systems

# Machine-level Programming
* Addressing modes
    * Normal: `(R)` -> `Mem[Reg[R]]`
    * Displacement: `D(R)` -> `Mem[Reg[R] + D]`
    * Complete: `D(Rb,Ri,S)` -> `Mem[Reg[Rb] + S*Reg[Ri] + D]`
        * `(Rb,Ri)` -> `Mem[Reg[Rb] + Reg[Ri]]`
        * `D(Rb,Ri)` -> `Mem[Reg[Rb] + Reg[Ri] + D]`
        * `(Rb,Ri,S)` -> `Mem[Reg[Rb] + S*Reg[Ri]]`
* Some instructions
    * `movq Src, Dst`
        * Cannot do memory-memory transfer with a single instruction
        * Intel docs use `mov Dst, Src`
    * `leaq Src, Dst`
        * Src is address mode expression, set Dst to address denoted by expression
        * Similar to `p = &x[i]`
        * Used for arithmetics for form like `x + k * y`
        * Does not change condition codes
    * `addq/subq Src, Dst`
    * `imulq Src, Dst`
    * `salq/sarq/shrq Src, Dst`
    * `xorq/andq/orq Src, Dst`
    * `pushq src`
    * `popq dest`
    * `incr dest`
* Compiler, Assembler, Linker & Loader  
    1. Compiler
        * Translates C files (.c) into assembly files (.s)
    2. Assembler
        * Translates assembly files (.s) into object files (.o)
        * Missing linkage between compilation units
    3. Linker
        * Resolve references between object files
        * Combine with static libraries (malloc, printf, etc)
    4. Dynamic linked libraries
        * Linking occurs at runtime
        * Does not take too much disk space  
    ![[compilation.png]]
* Data
    1. Arrays  
        * 1D arrays  
            ![[arrays.png]]
        * Nested 2D arrays: `int A[R][C]`
        ![[array_nested.png]]
        * Multi-level 2D arrays:  
            ![[array_multilevel.png]]
    2. Structs
        * Represented as block of memory
            ![[struct.png]]
        * Fields are ordered according to declaration
        * Alignment:  
            * Within struct: Each element has alignment requirement K, where K is the size of this element
                ![[alignment.png]]
            * Overall: Each struct has alignment requirement K, where K is the largest alignment of any element in struct
                ![[alignment_overall.png]]
            * To save space, put large data types first
    3. Float operations
        * Arguments passed in `%xmm0`, `%xmm1`, ...
        * Result returned in `%xmm0`
        * Different mov instructions are used to move floats
* Address space
    * Currently using 47-bit addresses (highest address of 0x7fffffffffff)
    * Maximum stack size of 8MB on most machines  
        ![[memory.png]]
* Vulnerablities
    1. Buffer overflow
        * Triggered by functions manipulating strings of arbitrary length
        * `gets`, `strcpy`, `strcat`, `scanf`, `fscanf`, `sscanf`
    2. Return-oriented programming (ROT)
        * Make use of "gadgets" in text segment  
        * Trigger with `ret` instruction  
            ![[rop.png]]
* Protection
    1. Use routines limiting string lengths (user-level)
    2. Randomized stack offsets
    3. Nonexecutable code segments
    4. Stack canaries

# Code optimization
* Optimization by programmer
    1. Code motion: Reduce frequency of computations performed   
        ![[code_motion.png]]
        GCC will do this with -O1  
    2. Reduction in strength: Reduce costly operation with simpler one  
        ![[reduction_in_strength.png]]
        Here, int mul requires 3 clock cycles, int add requires 1 clock cycle 
    3. Share common subexpressions  
        ![[share_common_subexpressions.png]]
* Optimization blockers
    1. Procedures: Seen as a "black box"
        * Procedures may have side effects
        * May not return same result with same argument
        * Fix: Use inline functions (GCC with -O1 within single file)
    2. Memory aliasing: Two memory references specify single location
        * The following code does memory load and store every time, because compiler assume possibility of memory aliasing:  
            ![[memory_aliasing.png]]
        * Load and store take multiple clock cycles
        * Easily caused by direct access to storage structures
        * Fix: Define local variable to tell compiler not to check for aliasing
            ![[aliasing_fix.png]]
        * Get in habit of introducing local variables accumulating within loops
* Optimization (by programmer) limitations
    1. Most performed within procedures. Newer versions of GCC do interprocedual optimization, but not between codes in different files
    2. Based on static information
    3. Conservative: Must not change program behavior
* Instruction-level parallelism
    * Superscalar processor: Issue and execute multuple instructions per cycle, and instructions are scheduled dynamically
    * Some instruction have >1 clock cycle latency, but can be pipelined:  
        ![[pipeline.png]]
    * Unrolling
        * Break sequential dependency to break through latency bound (to approach throughput bound)  
            ![[unrolling.png]]
            ```
            for(int i = 0; i < limit; ++i)
                x = x + d[i];
            ```
            can be optimized to:
            ```
            for(int i = 0; i < limit; i += 2)
                x = (x + d[i]) + d[i + 1];
            ```
            but to break sequential dependency:
            ```
            for(int i = 0; i < limit; i += 2)
                x = x + (d[i] + d[i + 1]);
            ``` 
        * adding separate accumulators
    * Branch prediction
        * Backward branches are often loops, predict taken
        * Forward branches are often if, predict not taken
        * Average better than 95% accuracy

# Memory
* Storage technologies
    1. RAMs
        * Volatile: SRAM & DRAM (caches & main memories)
        * Nonvolatile: ROM, PROM, EPROM, EEPROM (firmware, ssd & disk caches)
    2. Rotating disks
    3. SSDs
        * Page can be written only after its block has been erased
* Locality
    * Temporal locality
    * Spatial locality
* Hierarchy  
    ![[hierarchy.png]]
* Caches
    * Each level in hierarchy serves as cache for the level below
    * Types of cache misses
        1. Cold miss: "Warm up" cache
        2. Capacity miss: Working set larger than cache size
        3. Conflict miss: Limited by positioning restrictions imposed by hardware
    * Examples of cache  
        ![[cache_examples.png]]
* Cache memories
    * Concept of locality  
        ![[locality.png]]
    * General organization  
        ![[address.png]]
        ![[organization.png]]
        1. Direct mapped cache has (E / associativity = 1)  
            ![[direct_mapped_cache.png]]
        2. E-way set associative cache (Here E / associativity = 2)  
            ![[e_way_associative_cache.png]]
    * Metrics
        1. Miss rate
        2. Hit time
        3. Miss penalty
    * Write cache-friendly code
        1. Make the common cases go first
        2. Minimize the misses in inner loops
        3. Try to maximize spatial locality by reading objects sequentially with stride 1
        4. Try to maximize temporal locality by using an object as often as possible once it's read from memory
    * Example of matrix multiplication
        * In which order to arrange the loops? Do miss rate analysis!
        * It turns out: kij/ikj > ijk/jik > jki/kji
        * Use blocking: multiplying by sub-matrices

# Linking
* Why linkers?
    1. Modularity
    2. Efficiency (separate complilation)
* Two kind of linking
    1. Static linking
    2. Dynamic linking
* What does linker do?
    1. Symbol resolution
        * Functions, `global` vars, `static` vars
        * Definitions are stored in __symbol table__, an array of entries (name, size, location)
        * Three kind of symbols:
            1. Global symbols: non-static functions and non-static vars
            2. External symbols: defined in other modules
            3. Local symbols: static functions and static vars
            * **Note: Do not confuse local symbols with local variables.** Local variables are allocated in stack at runtime, and have nothing to do with linker. 
        * Symbol resolution
            * Symbols are strong or weak:
                1. Strong: functions and initialized globals
                2. Weak: uninitialized globals
            * Multiple strong symbols are not allowed
            * Choose the strong symbol over weak symbols
            * If there are multiple weak symbols, choose arbitrary one 
                * May cause undefined behavior over different compilers
                * Fix: use `static` and explicit `extern` 
    2. Relocation
        * Merge text and data segment into single sections
        * Relative location -> absolute location in .exe
        * Updates symbol table to their new positions
            * Relocation entries are used to aid symbol resolving:  
            `a: R_X86_64_32 array`
* Three kinds of object files (Modules)
    1. Relocatable object file (.o file)
    2. Executable object file (a.out file)
    3. Shared object file (.so file or .dll file)
* ELF format (Executable and Linkable Format)  
    * All 3 object files use ELF format  
    ![[elf.png]]
    ![[elf_2.png]]
* Static libraries (.a archive files)
    * Concatenate related relocatable object files into a single file with an index (called an archive)
    * During linking, only referenced .o files are linked
    * Command line order matters!
        * During scan, keep a list of currently unresolved references
        * If any entries in the unresolved list at end of scan, then error
        * Fix: put libraries at the end of command line
    * Commonly used libraries:
        * `libc.a` (the C standard library)
        * `limb.a` (the C math library)
    * Disadvantages
        * Duplication in storage
        * Bug fixes require relink
        * Fix: shared libraries
* Shared libraries
    * Dynamic linking can happen at:
        1. Load time
            * Handled by the dynamic linker
            * `libc.so` usually dynamically linked
        2. Run time
            * `dlopen()` interface in linux
* Library interpositioning
    * Can happen at:
        1. Compile time
        2. Link time
        3. Load/run time
    * Can be used for:
        1. Detecting memory leaks
        2. Generating address traces
    
# Exception Control Flows (ECF)
* ECFs exists in all levels:
    1. Exceptions (low level)
        * Processor responses to external events
        * Exception tables
    2. Context switch
    3. Signals
    4. Nonlocal jumps
* Exceptions (equivalent to user-kernel transition)  
    ![[exceptions.png]]
    1. Asynchronous (Interrupts)
        * Indicated by INT pin
        * Control flow returns to next instruction
    2. Synchronous
        1. Traps
            * Intentional (syscall, breakpoints)
            * Control flow returns to next instruction
        2. Faults
            * Unintentional but possibly recoverable
                * Processor transfer control to fault handler
                    * If correctable, returns control to the faulting instruction, thereby re-executing it
                    * Else, returns to an abort routine in the kernel that terminates the application that caused fault
            * Control flow returns to current instruction or aborts
        3. Aborts
            * Unintentional and unrecoverable
            * Never return control to the application program
            * Control flow returns to abort routine and aborts
        * ![[Pasted image 20230309132908.png]]
        * ![[Pasted image 20230309140323.png]]
* Context switches

# Processes
``pid_t getpid(void)``
- Returns PID of curr proccess
``pid_t getppid(void)``
- Returns PID of parent process
* From a programmer's perspective, a process can be:
    1. Running: Executing or will be scheduled
    2. Stopped: Suspended and will not be scheduled until further notice
    3. Terminated: Stopped permanently (zombie)
        * Process terminates when: 
            1. `SIGTERM` received
            2. Return from `main()`
            3. Called `exit()`
* Creating process: `fork()`
    * `fork()` called once but returns twice
        * Different values so you can tell which process is running - child or parent
        * Parent process: return value of ``fork()`` is **PID** of child process
        * Child process: return value of ``fork()`` is **0**
            * Duplicate but separate address spaces
            * Gets identitical copies of any of the parent's open file descriptors which means the child can read/write any files that were open in the parent when it called ``fork``
        * Seperate proccesses that run concurrently so the instructions are ran arbitrarily -- cannot make assumptions of the run order
    * `exit()` and `execve()` called once but possibly never returns
    * Control flow can be modelled with process graphs via toposort:
        ![[Pasted image 20230321140225.png]]
        ![[Pasted image 20230321140206.png]]
        ![[process_graph1.png]]
        ![[process_graph2.png]]
* Reaping child processes: `wait()`
    * Terminated processes become zombies, because its parent may use its exit status or OS tables
    * ![[Pasted image 20230323134442.png]]
    * `wait()` and `waitpid()` reap zombie child processes (even if not running, still consume system memory resources)
        * if the child exits before parent calls `wait()`, the call returns immediately as `wait` obtains info about child state and child termination is a changed state
        * Use `wait()` for single child process whereas `waitpid()` is meant for more complex programs with multiple children and parents or for zombie processes
    * If parent don't reap:
        1. If parent doesn't terminate: Never diminishes (a kind of memory leak)
        2. If parent does terminate: Reaped by `init` process (pid == 1) -- creating during start-up & never terminates & ancestor of every process
        * So only need to explicitly reap long-running processes such as **shells** or **servers**
* Loading and running processes: `execve()`
    * `int execve(char *filename, char *argv[], char *envp[])`
    * Loads and runs in the current process
    * Overwrites code, data and stack
    * Retains PID, open files (e.g. `stdout`), and signal context
    * Called once and never return (except error)
* Process groups
    * Can be get and set by `getpgrp()` and `setpgid()`
    * Kill all process in a group with `kill -n -<pid>` 


# Signals
* Unix shell: An application that runs program on behalf of the user
    * Shell contains a basic loop and a `eval()` function
    * Two cases in `eval()`:
        1. Shell built-in command
        2. Not build-in, use `fork()` and `execve()`
    * __Motivation__: How to reap __both__ foreground and background jobs?
        * Basic loop: Only reaps foreground jobs
        * Fix: Signals
* Signals
    * Akin to exceptions and interrupts
    * Sent from signal (sometimes at the request of another process via `kill`)
    * Identified by an integer
    * Controlled by __per-process__ `pending` and `blocked` bit vectors
        * `pending` vector set and cleared __by kernel__ when signals is sent or received
        * `blocked` vector can be manipulated by `sigprocmask()` function
        * So, signals cannot be queued
    * __Send__: `pending` bit set
    * __Receive__: process reacts to the signal, clears `pending` bit
        1. Ignore
        2. Terminate
        3. Catch (using user-level function called _signal handler_)
    * Kernels checks for `pnb = pending & ~blocked` at beginning of a time-slice
        * If `pnb == 0`:
            * Pass control to next instruction in the process logical flow 
        * Else
            1. Choose lease non-zero bit in `pnb` and forces the process to receive the signal
            2. The receipt of the signal triggers some action by the process (clears `pending` bit)
            3. Repeat for all remaining nonzero bits
            4. Pass control to next instruction in the process logical flow
    * Default action can be one of:
        1. Termination
        2. Stop until restarted by `SIGCONT`
        3. Ignore
    * Override default action by installing `signal handlers`:
        * `handler_t *signal(int signum, handler_t *handler)`
        * `handler` can be one of:
            1. `SIG_IGN`: Ignore
            2. `SIG_DFL`: Revert to default
            3. Function pointer to a user-level signal handler
    * Signal handlers are a form of concurrency  
        ![[concurrent_flows.png]]
        ![[concurrent_flows2.png]]
    * Signal handlers can be nested  
        ![[nested_signal_handlers.png]]
        * So we need __blocking__
            1. Implicit blocking: blocks pendings signals of same type
            2. Explicit blocking: `sigprogmask()` with supporting functions of:
                * `sigemptyset()`
                * `sigfillset()`
                * `sigaddset()`
                * `sigdelset()`
    * How to write safe handlers?
        1. Keep handlers as simple as possible
        2. Call only `async-signal-safe` function in handlers
            * `async-signal-safe` functions are _reentrent_ (access only local variables on stack), or cannot be interrupted by another signal handler
            * `printf()`, `malloc()` and `exit()` are __not__ safe
            * `write()` is the only signal-safe output function
        3. Save and restore `errno` on entry and exit
        4. Protect accesses to shared data structures by temporarily blocking __all__ signals in __both__ handler and `main()`
        5. Declare global variables to be `volatile`, to prevent from being optimized into registers
        6. Declare global __flags__ as `volatile sig_atomic_t`
            * Flag: variable only read or written (not `flag++` or `flag+=10`)
            * `volatile sig_atomic_t` are ints on most systems
    * Avoid race conditions
        * Cannot make `any` assumption regarding execution order
        * However, we can control when handlers run by blocking
    * Explicitly waiting for signals: suppose handler sets global variable `pid`:
        * Spin wait: `while(!pid) {}`
            * Wasteful
        * Pause: `while(!pid) pause()`
            * Race condition
        * Sleep: `while(!pid) sleep(1)`
            * Too slow
        * Solution: `sigsuspend`
            * `int sigsuspend(const sigset_t *mask)`
            * Equivalent to __atomic__:
                ```
                sigprocmask(SIG_BLOCK, &mask, &prev);
                pause()
                sigprocmask(SIG_BLOCK, &prev, NULL);    
                ```
    * Portable signal handling
        * Problem: Different versions of unix have different signal handling semantics
        * Solution: Use `sigaction`

# Virtual Memory
* Physical Addressing: Used in microcontrollers, embedded systems, etc.
* __Mentality__: Main memory is a fully-associative cache for disk
    * Load doesn't necessarily happen with `execve()`. It only allocates virtual address space with valid bit of 0
    * __Loading is a result of a page fault__ (demand paging)
* Kernel memory invisible to application program. Kernel's address space starts with 1.
* Every memory access go through cache memory:
    * Both memory and cache gets updated after page fault  
        ![[vm_and_cache.png]]
* Address translation: Multi-level page tables
* TLB: Small __set-associative__ hardware cache in MMU
* Works only because of locality

# System-Level I/O
* Unix I/O
    1. Opening and closing files: `open()`, `close()`
    2. Reading and writing files: `read()`, `write()`
    3. Changing file position: `lseek()`
    4. View file metadata: `stat()`
        * `stat()` are both a syscall and a linux program
        * Syscalls are in second section of man: `man 2 stat` 
    * Always check return codes for these syscalls
* File types: Regular, directory, socket, named pipes, symlinks, character and block devices
* Short counts: (`nbytes < sizeof(buf)`) are possible
* Wrapper: RIO (robust I/O) package
    1. Unbuffered I/O of binary data: `rio_readn()` and `rio_writen()`
    2. Buffered I/O of text or binary: `rio_readlineb()` and `rio_readnb()`
    * RIO package is better for input and output on network sockets
* Standard I/O
    1. Opening and closing: `fopen()` and `fclose()`
    2. Reading and writing bytes: `fread()` and `fwrite()`
    3. Reading and writing text lines: `fgets()` and `fputs()`
    4. Formatted reading and writing: `fscanf()` and `fprintf()`
    * C program begin with 3 open files:
        1. `stdin` (descriptor 0) 
        2. `stdout` (descriptor 1)
        3. `stderr` (descriptor 2)
* Trace syscalls with the Linux `strace` program
* Choosing I/O functions
    * General: Use highest-level functions
    * When to use Unix I/O: Signal handlers because unix I/O functions are `async-signal-safe`
    * When to use standard I/O: Disks, terminals
    * When to use RIO: Network sockets
* How kernel represents open files  
    ![[open_files1.png]]
    * Open file table: An instance of opening file
        * If a process opens a file twice, there are two open file tables pointing to the same v-node table
    * V-node table: File metadata (regardless of whether file is open)
    * After `fork()`, `refcnt` is incremented:  
        ![[open_files2.png]]
        * Two processes share a same instance of opened file (including file position)
    * `dup2(int oldfd, int newfd)`: Used for I/O redirection
        * After calling `dup2(4, 1)`:  
            ![[open_files3.png]]
* Recommended references:
    * W. Richard Stevens & Stephen A. Rago, _Advanced Programming in the Unix Environment_, 2 nd Edition, Addison Wesley, 2005

# Virtual Memory: Systems
* End-to-end Core i7 Address Translation  
    ![[address_translation.png]]
* L1 d-cache `index` and `offset` have 12 bits is NOT a coincidence: Speed up address translation  
    ![[cache_speedup.png]]
* Linux organizes VM as collections of areas:  
    * Fault handling: Traverse the `vm_area_struct`s to check if page is allocated
    ![[areas.png]]
* Private Copy-on-write (COW)
* Memory Mapping: `void *mmap(void *start, int len, int prot, int flags, int fd, int offset)`
    * `start`: A hint address
    * `prot`: `PROT_READ`, `PROT_WRITE`, `PROT_EXEC` 
    * `flags`: `MAP_ANON`, `MAP_PRIVATE`, `MAP_SHARED`
    * Returns a pointer to the start of mapped area (may not be start)

# Dynamic Memory Allocation
* Allocators: Maintain the heap as a collection of variable sized blocks, which are either _allocated_ or _free_
    * Explicit allocator: Application allocates and frees
        * E.g., ``malloc`` and ``free`` in C
    * Implicit allocator: Application allocates but not frees
        * E.g., garbage collection in Java, ML, and Lis
* The `malloc` package
    * `void *malloc(size_t size)`
        * Success - returns ptr to memory block of at least **size** bytes aligned to 8-byte or 16 byte boundary
            * If size == 0, returns ``NULL``
        * Unsuccessful - NULL and sets ``errno``
    * `void free(void *p)`
        * Returns block pointed at by ``p`` to pool of available memory
        *  `p` must come from a previous call to `malloc` / `realloc`
    * `calloc`: Initializes allocated blocks to 0
    * `realloc`: Changes size of previously allocated block
    * `sbrk`: Used internally by allocators to grow or shrink the heap
* Constraints:
    * Applications have few constraints
        * Can cause arbitrary sequence of `malloc` and `free` requests
        * Free request must be to a `malloc` block
    * Allocators have many constraints:
        1. Can't assume allocation patterns
        2. Must respond immediately to `malloc` (can't defer allocation)
        3. Can't relocate allocated memory
        4. Can manipulate and modify only free memory
        5. Can't move the allocated blocks once they are malloc'd
            1. compaction is not allowed
        6. Must allign blocks so they satisfy all alignment requirements
* Performance goal (2 conflicting goals)
    * Throughput: Number of completed requests per unit time
    * Peak memory utilization: How to efficiently use memory
* Fragmentation
    * Internal fragmentation: 
        * Payload smaller than block size
        * Easy to measure
    * External fragmentation: 
        * Enough aggregate heap memory, but no single free block large enough
        * Difficult to measure
* Keeping track of free blocks  
    ![[keeping_track_of_free_lists.png]]
* How to find a free block
    1. First fit
    2. Next fit
    3. Best fit
* Know how much to free: Header
    * Encodes block size (including the header and any padding) 
    * Alignment means lower-bits of size are 0, used to encode allocated bit  
        ![[implicit_free_list.png]]
* Implicit list
    * Allocating a free block: May need to split the block
    * Freeing a block: Have to coalesce free blocks (4 cases):  
        ![[coalescing.png]]
    * Singly-linked list cannot free previous block in constant time
        * Fix: Doubly-linked list (head and footer)
        * Optimization: 
            * Allocated blocks doesn't need coalescing
            * We have extra bits to encode whether previous block is allocated  
            * So, allocated blocks doesn't need footer -- **only need when previous block is free**
    * Implicit lists are not commonly used because of linear time. However, the concepts of splitting and coalescing are general to __all__ allcators
    * Boundary Tags:
        * Used for combining adjacent free blocks of memory into a larger block of memory
        * Downsides:
            * Memory overhead
            * Internal fragmentation
            * Limited scalability
* Explicit free list  
    ![[explicit_free_list.png]]
    * Maintain list of free blocks using payload area
    * Blocks can be in any order (depending on insertion policy)
        * Unordered: LIFO, FIFO
        * Address-ordered
    * Much faster than implicit lists when memory is full
* Segregated list  
    ![[segregated_list.png]]
* Garbage collection  
    ![[memory_graph.png]]
    * Mark and sweep collecting
        * Allocate using `malloc` until run out of space
        * Use extra mark bit for each block
        * Root nodes: Pointers in stack/data section
        * Does not distinguish between pointers/non-pointers, thus "safe"
        * _Mark_: Start at root nodes and do DFS
        * _Sweep_: Start at beginning of VM, and free unmarked blocks
        * How to find beginning of block? -- Use a balanced tree

# Memory-Related Perils/Pitfalls
- Dereferencing Bad Pointers
	- classic ``scanf`` bug![[Pasted image 20230307140113.png]]
- Reading Uninitialized Memory
	- Assuming that heap data is initialized to zero![[Pasted image 20230307140200.png]]
- Overwriting Memory
	- Allocating the (possibly) wrong sized object ![[Pasted image 20230307141002.png]]
		- `int` is incorrect, should be `N*sizeof(p[0])` because it is more dynamic
			- Could also be just `sizeof(int*)`
		- Memory leak
		- Need to check if malloc is successful
	- Off-by-one error ![[Pasted image 20230307141157.png]]
		- Loops `0` to `N` instead of `0` to `N-1`
	- ![[Pasted image 20230307141330.png]]
		- Not checking max string size
		- Basis for classic buffer overflow attacks
		- Never use `gets()`
		- The rest of the read-in info goes into memory and goes god knows where
		- Malicious programmers could inject malicious contents into program memory
			- Hence buffer overflow
	- Misunderstanding pointer arithmetic![[Pasted image 20230307142328.png]]
	- Referencing a pointer instead of the object it poibts to ![[Pasted image 20230307142338.png]]
		- Supposed to delete minimum element from a binary heap data structure (minimum element is typically at root of the heap -- first element in the array)
		- `packet` points to the minimum element
			- Replaces the minimum element with the last element in the heap then decrements size ptr to reflect the removal. Lastly, it restores binary heap by calling `Heapify`
		- `*size--*` is misleading as you are decrementing ptr rather than the value 
			- need `--*size*`
	- Referencing Nonexistent Variables ![[Pasted image 20230307143238.png]]
		- Forgetting that local variables disappear when a function returns
		- Local variables in a function stored on stack and deallocated when function returns
	- Freeing Blocks Multiple Times ![[Pasted image 20230307143444.png]]
	- 
# Network Programming
* Client-Server Architecture
* Network Architecture
    * Routers set borders of LANs
    * Conceptual view of LANs:  
        ![[LAN.png]]
* Socket
    * To the kernel: An endpoint of communication
    * To application: A file descriptor to write/read
    * Generic socket address:  
        ![[socket_generic.png]]
    * IPv4 specific socket address:  
        ![[socket_ipv4.png]]
* Host and Service Conversion: `getaddrinfo`
    * Convert string representations of hostnames, host addresses, ports and service names to socket address structures  
        ![[getaddrinfo.png]]
        ![[addrinfo.png]]
    * `getnameinfo` is the inverse of `getaddrinfo`
* Client/Server interface  
    ![[client_server.png]]
* `telnet`: Testing servers
    * Creates TCP connection with a server (starts a session)
    * Since the encoding of HTTP is ascii, we can hard-code http requests
* HTTP
    * Content: A sequence of bytes in MIME (Multipurpose Internet Mail Extension) type
    * The contents can be either _static_ or _dynamic_
* Dynamic content: 
    * Produced by server-side program
    * If URI containts `cgi-bin` then serve dynamic content
    * Use `fork()` and `exec()` to execute new program
    * Use env-var `QUERY_STRING` to pass parameters
# Concurrency
* Iterative servers have serious flaws. 
    * Easily get blocked by single misbehaving client
        * Note: Blocking does not happen upon client calling `connect()` or `write()`, but upon `read()`. This is because server's kernel provides buffering
    * So we need concurrent servers
1. Process-based servers
    * Parent __must__ close connected socket (parent doesn't get reaped)
    * Child __should__ close listening socket (child gets reaped)
    * Reap child with `SIGCHLD` handler
2. Event-based servers
    * Manage multiple connections in user space 
    * Determine events using `select()` or `epoll()`
    * Design of choice for high-performance web servers
    * However, hard to provide find-grained concurrency
    * Cannot take advantage of multi-core
3. Thread-based servers
    * Can run threads in `detached` mode. It will run independently, and get reaped automatically
    * Possible race conditions when passing parameters to new thread in `pthread_create()`